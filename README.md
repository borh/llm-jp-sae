# LLM-jp SAE
(Work in progress)
This repository provides code for training and evaluating Sparse Autoencoders (SAEs) on the internal representations of LLM-jp.
We trained SAEs separately on six different checkpoints of [LLM-jp-3-1.8B](https://huggingface.co/llm-jp/llm-jp-3-1.8b) and compared learned features across checkpoints.

- **[Demo Page](https://llm-jp.github.io/llm-jp-sae/)**: Visualize the text samples that activate each SAE feature (100 features per checkpoint).
- **[Model Weights](https://huggingface.co/llm-jp)**: SAE weights for all six checkpoints.
- **[Paper](https://arxiv.org/)**: *"How LLMs Learn: Tracing Internal Representations with Sparse Autoencoders"*

